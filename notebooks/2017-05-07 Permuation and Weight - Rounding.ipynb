{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.python import debug as tf_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n",
      "[[ 0.10077184  1.6843127   0.4835811  -1.09100914  0.13907139]\n",
      " [-0.83964658  0.80931187 -1.2240566   0.80600601 -1.37996602]\n",
      " [ 0.71245444 -0.23467234  0.44525203 -0.87935728  0.42449108]\n",
      " [ 0.24505809  0.16287078  1.39144504  0.97881103  0.3398042 ]\n",
      " [-1.22279549 -1.71691489 -0.85684156  0.59503055  1.17687225]]\n"
     ]
    }
   ],
   "source": [
    "#Generate an underlying permutation matrix.\n",
    "# Global parameters\n",
    "#np.random.seed(12)\n",
    "#Permutation and weight matrices are KxK\n",
    "sess = tf.Session()\n",
    "#sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "#sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "\n",
    "K = 5\n",
    "num_samples = 10\n",
    "#Don't have an annealing schedule\n",
    "tau = tf.constant(0.75)\n",
    "# Sample a true permutation (in=col, out=row)\n",
    "#perm_true = np.random.permutation(np.arange(K))\n",
    "#P_true = np.zeros((K, K))\n",
    "#P_true[np.arange(K), perm_true] = 1\n",
    "#P_true = np.float32(P_true)\n",
    "P_true = np.eye(K)\n",
    "print(P_true)\n",
    "#Generate an underlying weight matrix.\n",
    "W_true = np.float32(np.random.randn(K,K))\n",
    "print(W_true)\n",
    "#Generate ficticious data from the permutation and weight matrix.\n",
    "#x_i = PWP^T*a_i+epsilon, i.e. x_i~N(PWP^T*a_i,sigma_true) for epsilon~N(0,sigma_true)\n",
    "noise_scale = 0.1\n",
    "RChol = np.array(np.random.rand(K,K)*noise_scale,dtype='float32')\n",
    "sigma_true = RChol.dot(RChol.T)\n",
    "#Now generate the a_i.\n",
    "a = np.array(np.random.rand(num_samples,K,1),dtype='float32')\n",
    "#Now the data x that will be observed.\n",
    "PWP = P_true.dot(W_true).dot(P_true.T)\n",
    "x = np.matmul(PWP,a)\n",
    "x = tf.constant(np.float32(x))\n",
    "a = tf.constant(np.float32(a))\n",
    "#Could be a parameter to be learned, but set to constant for now.\n",
    "sigmaChol = np.array(np.random.rand(K,K)*noise_scale,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Variational model needs to return both a permutation sample P, and a weight sample W\n",
    "#Weight sample W for now will be independent normals for each Wij.\n",
    "#P is according to our rounding method.\n",
    "\n",
    "mu_W = tf.Variable(np.array(np.ones([K,K]),dtype='float32'))\n",
    "#Initialise small noise leads to better training results in my experience.\n",
    "sigma_W = tf.Variable(np.array(np.ones([K,K])*0.5,dtype='float32'))\n",
    "#Comp stability hacks. Change to tau?\n",
    "sigma_W_pos = sigma_W**2+10E-5\n",
    "tf.assert_positive(sigma_W_pos)\n",
    "#Define a tensorflow matrix of independence normal random variables for the variational posterior.\n",
    "q_W = tf.contrib.distributions.Normal(loc=mu_W, scale=sigma_W_pos)\n",
    "\n",
    "#Now want to define the variational posterior for the permutation.\n",
    "mu_P = tf.Variable(np.array(np.random.rand(K,K),dtype='float32'))\n",
    "mu_P = mu_P**2+10E-5\n",
    "tf.assert_positive(mu_P)\n",
    "\n",
    "\n",
    "#Scott's Sinkhorn in logspace\n",
    "def sinkhorn_logspace(logP, niters=3):\n",
    "    for _ in range(niters):\n",
    "        # Normalize columns and take the log again\n",
    "        logP = logP - tf.reduce_logsumexp(logP, axis=0,keep_dims=True)\n",
    "        # Normalize rows and take the log again\n",
    "        logP = logP - tf.reduce_logsumexp(logP, axis=1,keep_dims=True)\n",
    "    return logP\n",
    "#Remember to exp() again to get the matrix out of log-space.    \n",
    "mu_P_sinkhorned = tf.exp(sinkhorn_logspace(tf.log(mu_P),niters=5))\n",
    "#vectorize the matrix\n",
    "mu_P_vec = tf.reshape(mu_P_sinkhorned,shape=[-1])\n",
    "#eta_P is the diagonal of a matrix that we draw permutation samples from.\n",
    "eta_P = tf.Variable(np.array(np.random.rand(K**2,1),dtype='float32'))\n",
    "#The bounds that seem to be suggested in Gonzalo's code.\n",
    "lb = 0.001\n",
    "ub = 0.01\n",
    "#Enforcing the bounds.\n",
    "eta_P_constrained = tf.reshape(lb+(ub-lb)*tf.nn.sigmoid(eta_P),shape=[-1])\n",
    "tf.assert_positive(eta_P_constrained)\n",
    "#Creating another tensorflow normal distribution according to the model discussed on\n",
    "#slack.\n",
    "q_P = tf.contrib.distributions.Normal(loc=mu_P_vec,scale = eta_P_constrained)\n",
    "def variational_sample():\n",
    "    #Works by default for the tensorflow distribution object\n",
    "    W_sample = q_W.sample()\n",
    "    #Now need a permutation sample. Start by reshaping hte draw from q_P into a KxK matrix\n",
    "    phi = tf.reshape(q_P.sample(),shape=[K,K])\n",
    "    #Now have to round using Hungarian. But there is no tensorflow implementation. So we evaluate\n",
    "    #using sess.run, and run the standard numpy. It's probably better to use \"with Session as sess\" etc and indent\n",
    "    #for all this, but that makes it more difficult to use the jupyter notebook cells.\n",
    "    #So now just generate the sample as in Gonzalo's notebook.\n",
    "    row, col = optimize.linear_sum_assignment(-sess.run(phi))\n",
    "    rounded_phi = np.zeros((K, K))\n",
    "    rounded_phi[row, col] = 1.0\n",
    "    rounded_phi =  tf.constant(np.float32(rounded_phi))\n",
    "    P_sample = tau*phi+(1-tau)*rounded_phi\n",
    "    #Remember returns the sample for both parts of the variational posterior.\n",
    "    #The following two lines can be used to check inference on each parameter individually.\n",
    "    #P = tf.constant(P_true)\n",
    "    #W = tf.constant(W_true)\n",
    "    return [W_sample,P_sample]\n",
    "\n",
    "#This gives a single sample approximation for the expectation of the log prob of q(z|x).\n",
    "def variational_log_prob(W_sample):\n",
    "    #Log probability of the sample of W\n",
    "    var_log_prob = tf.reduce_sum(q_W.log_prob(W_sample))\n",
    "    #Log probability of the sample of P, just the entropy of the normal from the sampling process. Handy.\n",
    "    entropy_phi = q_P.entropy()\n",
    "    return var_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to create these now to allow for the Hungarian work-around to work. Drawing samples requires the parameters to\n",
    "#be defined because Hungarian requires actual values. But we have to initialise again later to define the optimization\n",
    "#related parameters.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "W_sample,P_sample = variational_sample()\n",
    "#Need to be careful with training algorithm and learning rate. Despite precautions, seem to veer into NaN territory\n",
    "#at times.\n",
    "#optimizer = tf.train.AdagradOptimizer(learning_rate = 1.0)\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "#optimizer = tf.train.FtrlOptimizer(learning_rate = 0.5)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.002)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate = 0.001,momentum = 0.01)\n",
    "#optimizer = tf.train.ProximalAdagradOptimizer(learning_rate=0.0001)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logpdf for multivariate normal since tensorflows mvn distribution is currently limited.\n",
    "def tf_mvn_logpdf(X,Mu,XChol):\n",
    "    ''' Use this version when X is a vector '''\n",
    "    Lambda = tf.matrix_inverse(tf.matmul(XChol,tf.transpose(XChol)))\n",
    "    #Lambda = tf.matmul(XChol,tf.transpose(XChol))\n",
    "    XMu    = X-Mu\n",
    "    return_value= (-0.5 * tf.matmul(XMu, tf.matmul(Lambda,tf.transpose(XMu)))\n",
    "                  +0.5 * tf.log(tf.matrix_determinant(Lambda))\n",
    "                  -0.5 * np.log(2*np.pi) * int(X.shape[1]))       \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We assume we know both x and a, with both P and W have uniform prior distributions\n",
    "def generative_log_prob(x,a,P,W,sigmaChol):\n",
    "    #Here x and a are data, but P and W are tensorflow variable samples from the variational posterior\n",
    "    #In accordance with x_i~N(PWP^T*a_i,sigma_true)\n",
    "    A = tf.matmul(P,tf.matmul(W,tf.transpose(P)))\n",
    "    #Below just encodes the prior over the x's. Simply repeatedly call tf_mvn_logpdf for each sample. Can do this\n",
    "    #quicker for a tf_mvn_logpdf that acts on a matrix of X's rather than a vector, but I didn't have time to reimpl\n",
    "    #ement int.\n",
    "    indices = tf.constant(np.arange(num_samples,dtype='int32'))\n",
    "    def mvn_calls(index):\n",
    "        return tf_mvn_logpdf(tf.transpose(x[index]),tf.transpose(tf.matmul(A,a[index])),sigmaChol)\n",
    "    gen_log_prob = tf.reduce_sum(tf.map_fn(mvn_calls,indices,dtype='float32'))    \n",
    "    #Prior on normals to be standard uniform.\n",
    "    W_check = tf.contrib.distributions.Normal(loc=np.array(np.zeros([K,K]),dtype='float32'),\n",
    "                                              scale = np.array(np.ones([K,K]),dtype='float32'))\n",
    "    gen_log_prob = gen_log_prob + tf.reduce_sum(W_check.log_prob(W))\n",
    "    return gen_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10077184  1.6843127   0.4835811  -1.09100914  0.13907139]\n",
      " [-0.83964658  0.80931187 -1.2240566   0.80600601 -1.37996602]\n",
      " [ 0.71245444 -0.23467234  0.44525203 -0.87935728  0.42449108]\n",
      " [ 0.24505809  0.16287078  1.39144504  0.97881103  0.3398042 ]\n",
      " [-1.22279549 -1.71691489 -0.85684156  0.59503055  1.17687225]]\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  1.]]\n"
     ]
    }
   ],
   "source": [
    "#Single sample approximation of the ELBO.\n",
    "ELBO = generative_log_prob(x,a,P_sample,W_sample,sigmaChol)-variational_log_prob(W_sample)\n",
    "print(W_true)\n",
    "print(P_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-25541.188]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "100 [-4448.3242]\n",
      "[array([[ 0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.]], dtype=float32)]\n",
      "200 [-2277.355]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "300 [-1263.4819]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "400 [-753.57898]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "500 [-536.74335]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "600 [-270.21991]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "700 [-206.51067]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "800 [-146.87857]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "900 [-112.01054]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1000 [-101.33205]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1100 [-90.318283]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1200 [-72.368584]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1300 [-69.023392]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1400 [-74.039398]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1500 [-66.690903]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1600 [-46.904518]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1700 [-51.876606]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1800 [-76.092728]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "1900 [-34.178371]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "2000 [-27.894115]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "2100 [-31.241795]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "2200 [-61.148857]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "2300 [-29.805538]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "2400 [-13.604935]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "2500 [-26.698837]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n",
      "2600 [-21.398727]\n",
      "[array([[ 0.,  0.,  1.,  0.,  0.],\n",
      "       [ 0.,  1.,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0.,  1.,  0.],\n",
      "       [ 0.,  0.,  0.,  0.,  1.]], dtype=float32)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b82dabad6b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mgrad_check_print\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_checker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mmu_W_print\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma_W_print\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu_P_print\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta_P_print\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmu_W\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msigma_W\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmu_P\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meta_P\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mascent_tracker\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mELBO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scott/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scott/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scott/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/scott/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/scott/anaconda3/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# gradients, variables = zip(*optimizer.compute_gradients(-ELBO))\n",
    "# gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "# train_fn = optimizer.apply_gradients(zip(gradients, variables))\n",
    "train_fn = optimizer.minimize(-ELBO)\n",
    "train_steps=50000\n",
    "grad_checker = tf.gradients(-ELBO,tf.trainable_variables())\n",
    "#check_op = tf.add_check_numerics_ops()\n",
    "#Need to run initializer twice to allow for the optimizer to be instantiated. But need to run it earlier in\n",
    "#order to get around the lack of a Tensorflow hungarian algorithm.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#####\n",
    "ascent_tracker = np.array([np.nan]*train_steps)\n",
    "for step in range(train_steps):\n",
    "    grad_check_print = sess.run(grad_checker)\n",
    "    mu_W_print,sigma_W_print,mu_P_print,eta_P_print = sess.run([mu_W,sigma_W,mu_P,eta_P])\n",
    "    sess.run([train_fn])\n",
    "    ascent_tracker[step]=sess.run(ELBO)\n",
    "    if (step%100==0):\n",
    "        print(step, sess.run([ELBO]))\n",
    "        print(sess.run([tf.round(P_sample)]))\n",
    "        if np.isnan(sess.run(ELBO)):\n",
    "            break\n",
    "        #print(sess.run(grad_checker))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
