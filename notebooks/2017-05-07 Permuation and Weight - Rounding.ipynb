{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "from scipy import optimize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tensorflow.python import debug as tf_debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Generate an underlying permutation matrix.\n",
    "# Global parameters\n",
    "#np.random.seed(12)\n",
    "#Permutation and weight matrices are KxK\n",
    "sess = tf.Session()\n",
    "#sess = tf_debug.LocalCLIDebugWrapperSession(sess)\n",
    "#sess.add_tensor_filter(\"has_inf_or_nan\", tf_debug.has_inf_or_nan)\n",
    "\n",
    "K = 5\n",
    "num_samples = 10\n",
    "#Don't have an annealing schedule\n",
    "tau = tf.constant(0.75)\n",
    "# Sample a true permutation (in=col, out=row)\n",
    "#perm_true = np.random.permutation(np.arange(K))\n",
    "#P_true = np.zeros((K, K))\n",
    "#P_true[np.arange(K), perm_true] = 1\n",
    "#P_true = np.float32(P_true)\n",
    "P_true = np.eye(K)\n",
    "print P_true\n",
    "#Generate an underlying weight matrix.\n",
    "W_true = np.float32(np.random.randn(K,K))\n",
    "print W_true\n",
    "#Generate ficticious data from the permutation and weight matrix.\n",
    "#x_i = PWP^T*a_i+epsilon, i.e. x_i~N(PWP^T*a_i,sigma_true) for epsilon~N(0,sigma_true)\n",
    "noise_scale = 0.1\n",
    "RChol = np.array(np.random.rand(K,K)*noise_scale,dtype='float32')\n",
    "sigma_true = RChol.dot(RChol.T)\n",
    "#Now generate the a_i.\n",
    "a = np.array(np.random.rand(num_samples,K,1),dtype='float32')\n",
    "#Now the data x that will be observed.\n",
    "PWP = P_true.dot(W_true).dot(P_true.T)\n",
    "x = np.matmul(PWP,a)\n",
    "x = tf.constant(np.float32(x))\n",
    "a = tf.constant(np.float32(a))\n",
    "#Could be a parameter to be learned, but set to constant for now.\n",
    "sigmaChol = np.array(np.random.rand(K,K)*noise_scale,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Variational model needs to return both a permutation sample P, and a weight sample W\n",
    "#Weight sample W for now will be independent normals for each Wij.\n",
    "#P is according to our rounding method.\n",
    "\n",
    "mu_W = tf.Variable(np.array(np.ones([K,K]),dtype='float32'))\n",
    "#Initialise small noise leads to better training results in my experience.\n",
    "sigma_W = tf.Variable(np.array(np.ones([K,K])*0.5,dtype='float32'))\n",
    "#Comp stability hacks. Change to tau?\n",
    "sigma_W_pos = sigma_W**2+10E-5\n",
    "tf.assert_positive(sigma_W_pos)\n",
    "#Define a tensorflow matrix of independence normal random variables for the variational posterior.\n",
    "q_W = tf.contrib.distributions.Normal(loc=mu_W, scale=sigma_W_pos)\n",
    "\n",
    "#Now want to define the variational posterior for the permutation.\n",
    "mu_P = tf.Variable(np.array(np.random.rand(K,K),dtype='float32'))\n",
    "mu_P = mu_P**2+10E-5\n",
    "tf.assert_positive(mu_P)\n",
    "\n",
    "\n",
    "#Scott's Sinkhorn in logspace\n",
    "def sinkhorn_logspace(logP, niters=3):\n",
    "    for _ in range(niters):\n",
    "        # Normalize columns and take the log again\n",
    "        logP = logP - tf.reduce_logsumexp(logP, axis=0,keep_dims=True)\n",
    "        # Normalize rows and take the log again\n",
    "        logP = logP - tf.reduce_logsumexp(logP, axis=1,keep_dims=True)\n",
    "    return logP\n",
    "#Remember to exp() again to get the matrix out of log-space.    \n",
    "mu_P_sinkhorned = tf.exp(sinkhorn_logspace(tf.log(mu_P),niters=5))\n",
    "#vectorize the matrix\n",
    "mu_P_vec = tf.reshape(mu_P_sinkhorned,shape=[-1])\n",
    "#eta_P is the diagonal of a matrix that we draw permutation samples from.\n",
    "eta_P = tf.Variable(np.array(np.random.rand(K**2,1),dtype='float32'))\n",
    "#The bounds that seem to be suggested in Gonzalo's code.\n",
    "lb = 0.001\n",
    "ub = 0.01\n",
    "#Enforcing the bounds.\n",
    "eta_P_constrained = tf.reshape(lb+(ub-lb)*tf.nn.sigmoid(eta_P),shape=[-1])\n",
    "tf.assert_positive(eta_P_constrained)\n",
    "#Creating another tensorflow normal distribution according to the model discussed on\n",
    "#slack.\n",
    "q_P = tf.contrib.distributions.Normal(loc=mu_P_vec,scale = eta_P_constrained)\n",
    "def variational_sample():\n",
    "    #Works by default for the tensorflow distribution object\n",
    "    W_sample = q_W.sample()\n",
    "    #Now need a permutation sample. Start by reshaping hte draw from q_P into a KxK matrix\n",
    "    phi = tf.reshape(q_P.sample(),shape=[K,K])\n",
    "    #Now have to round using Hungarian. But there is no tensorflow implementation. So we evaluate\n",
    "    #using sess.run, and run the standard numpy. It's probably better to use \"with Session as sess\" etc and indent\n",
    "    #for all this, but that makes it more difficult to use the jupyter notebook cells.\n",
    "    #So now just generate the sample as in Gonzalo's notebook.\n",
    "    row, col = optimize.linear_sum_assignment(-sess.run(phi))\n",
    "    rounded_phi = np.zeros((K, K))\n",
    "    rounded_phi[row, col] = 1.0\n",
    "    rounded_phi =  tf.constant(np.float32(rounded_phi))\n",
    "    P_sample = tau*phi+(1-tau)*rounded_phi\n",
    "    #Remember returns the sample for both parts of the variational posterior.\n",
    "    #The following two lines can be used to check inference on each parameter individually.\n",
    "    #P = tf.constant(P_true)\n",
    "    #W = tf.constant(W_true)\n",
    "    return [W_sample,P_sample]\n",
    "\n",
    "#This gives a single sample approximation for the expectation of the log prob of q(z|x).\n",
    "def variational_log_prob(W_sample):\n",
    "    #Log probability of the sample of W\n",
    "    var_log_prob = tf.reduce_sum(q_W.log_prob(W_sample))\n",
    "    #Log probability of the sample of P, just the entropy of the normal from the sampling process. Handy.\n",
    "    entropy_phi = q_P.entropy()\n",
    "    return var_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Need to create these now to allow for the Hungarian work-around to work. Drawing samples requires the parameters to\n",
    "#be defined because Hungarian requires actual values. But we have to initialise again later to define the optimization\n",
    "#related parameters.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "W_sample,P_sample = variational_sample()\n",
    "#Need to be careful with training algorithm and learning rate. Despite precautions, seem to veer into NaN territory\n",
    "#at times.\n",
    "#optimizer = tf.train.AdagradOptimizer(learning_rate = 1.0)\n",
    "optimizer = tf.train.AdamOptimizer(0.01)\n",
    "#optimizer = tf.train.FtrlOptimizer(learning_rate = 0.5)\n",
    "#optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.002)\n",
    "#optimizer = tf.train.MomentumOptimizer(learning_rate = 0.001,momentum = 0.01)\n",
    "#optimizer = tf.train.ProximalAdagradOptimizer(learning_rate=0.0001)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#logpdf for multivariate normal since tensorflows mvn distribution is currently limited.\n",
    "def tf_mvn_logpdf(X,Mu,XChol):\n",
    "    ''' Use this version when X is a vector '''\n",
    "    Lambda = tf.matrix_inverse(tf.matmul(XChol,tf.transpose(XChol)))\n",
    "    #Lambda = tf.matmul(XChol,tf.transpose(XChol))\n",
    "    XMu    = X-Mu\n",
    "    return_value= (-0.5 * tf.matmul(XMu, tf.matmul(Lambda,tf.transpose(XMu)))\n",
    "                  +0.5 * tf.log(tf.matrix_determinant(Lambda))\n",
    "                  -0.5 * np.log(2*np.pi) * int(X.shape[1]))       \n",
    "    return return_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We assume we know both x and a, with both P and W have uniform prior distributions\n",
    "def generative_log_prob(x,a,P,W,sigmaChol):\n",
    "    #Here x and a are data, but P and W are tensorflow variable samples from the variational posterior\n",
    "    #In accordance with x_i~N(PWP^T*a_i,sigma_true)\n",
    "    A = tf.matmul(P,tf.matmul(W,tf.transpose(P)))\n",
    "    #Below just encodes the prior over the x's. Simply repeatedly call tf_mvn_logpdf for each sample. Can do this\n",
    "    #quicker for a tf_mvn_logpdf that acts on a matrix of X's rather than a vector, but I didn't have time to reimpl\n",
    "    #ement int.\n",
    "    indices = tf.constant(np.arange(num_samples,dtype='int32'))\n",
    "    def mvn_calls(index):\n",
    "        return tf_mvn_logpdf(tf.transpose(x[index]),tf.transpose(tf.matmul(A,a[index])),sigmaChol)\n",
    "    gen_log_prob = tf.reduce_sum(tf.map_fn(mvn_calls,indices,dtype='float32'))    \n",
    "    #Prior on normals to be standard uniform.\n",
    "    W_check = tf.contrib.distributions.Normal(loc=np.array(np.zeros([K,K]),dtype='float32'),\n",
    "                                              scale = np.array(np.ones([K,K]),dtype='float32'))\n",
    "    gen_log_prob = gen_log_prob + tf.reduce_sum(W_check.log_prob(W))\n",
    "    return gen_log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Single sample approximation of the ELBO.\n",
    "ELBO = generative_log_prob(x,a,P_sample,W_sample,sigmaChol)-variational_log_prob(W_sample)\n",
    "print W_true\n",
    "print P_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gradients, variables = zip(*optimizer.compute_gradients(-ELBO))\n",
    "# gradients, _ = tf.clip_by_global_norm(gradients, 1.0)\n",
    "# train_fn = optimizer.apply_gradients(zip(gradients, variables))\n",
    "train_fn = optimizer.minimize(-ELBO)\n",
    "train_steps=50000\n",
    "grad_checker = tf.gradients(-ELBO,tf.trainable_variables())\n",
    "#check_op = tf.add_check_numerics_ops()\n",
    "#Need to run initializer twice to allow for the optimizer to be instantiated. But need to run it earlier in\n",
    "#order to get around the lack of a Tensorflow hungarian algorithm.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#####\n",
    "ascent_tracker = np.array([np.nan]*train_steps)\n",
    "for step in range(train_steps):\n",
    "    grad_check_print = sess.run(grad_checker)\n",
    "    mu_W_print,sigma_W_print,mu_P_print,eta_P_print = sess.run([mu_W,sigma_W,mu_P,eta_P])\n",
    "    sess.run([train_fn])\n",
    "    ascent_tracker[step]=sess.run(ELBO)\n",
    "    if (step%100==0):\n",
    "        print(step, sess.run([ELBO]))\n",
    "        print(sess.run([P_sample]))\n",
    "        if np.isnan(sess.run(ELBO)):\n",
    "            break\n",
    "        #print(sess.run(grad_checker))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
