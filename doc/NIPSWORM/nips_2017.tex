\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading nips_2017
%
% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{nips_2017}
\usepackage{nips_2017}

\input{preamble.tex}
\input{preamble_math.tex}
\input{preamble_acronyms.tex}
\usepackage{makecell}
\usepackage{blindtext}
\DeclareRobustCommand{\parhead}[1]{\textbf{#1}~}



% to compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{nips_2017}
\usepackage{graphicx}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Identifying neurons in \textit{C. elegans} with continuous relaxations for Bayesian permutation inference}

% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  David S.~Hippocampus\thanks{Use footnote for providing further
    information about author (webpage, alternative
    address)---\emph{not} for acknowledging funding agencies.} \\
  Department of Computer Science\\
  Cranberry-Lemon University\\
  Pittsburgh, PA 15213 \\
  \texttt{hippo@cs.cranberry-lemon.edu} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}
% \nipsfinalcopy is no longer used

\maketitle

\begin{abstract}
  The nematode C. elegans is a unique model organism for
  neuroscientists as its connectome, or neural wiring diagram, has
  been known for at least three decades. Despite this knowledge, an
  understanding of the functional significance of these synaptic
  connections has remained elusive. Now several groups can routinely
  image the activity of a large fraction of neurons in the head of the
  worm, providing a unique opportunity to probe this organism. We
  propose a hierarchical Bayesian framework that combines strong
  prior information with data from many experiments to estimate
  posteriors over the functional connectivity weights. However, these
  attempts are stifled by a major obstacle: in many cases it is not
  clear exactly which neurons are being imaged, so to combine
  information across experiments one must solve a matching, or
  permutation inference, problem.

  In this work we introduce new variational methods that enable the
  joint inference of connectivity weights and neural identity. Working
  with actual permutations would involve evaluating and
  differentiating an intractable partition function. As an
  alternative, we build upon recent continuous relaxation techniques
  \citep{Jang2016, Maddison2016}, extending them from the
  original case of the probability simplex, to the Birkhoff polytope,
  the convex hull of permutation matrices. We test our method with
  simulated data from the true connectome and known
  covariates (neural position) and show that our approach outperforms
  many alternatives in identifying neurons.
\end{abstract}



\section{Introduction}
The nematode \textit{C. elegans} plays a special role as a model
organism in neuroscience since its neural network is stereotyped from
animal to animal and its complete neural wiring diagram is
known~\citep{varshney2011structural}.  Modern calcium imaging
technology enables measurements of hundreds of these neurons
simultaneously \citep{Kato2015, nguyen2016whole}. The time is
right to employ modern statistical methods to learn about the
functional connectome in this system and suggest new experiments.

\begin{figure*}[t]
  \centering
  \includegraphics[width=5.0in]{Figure1.pdf} 
  \caption{\textit{Hierarchical Bayesian framework}.  \textbf{A} We
    are given the actual adjaency matrix $A$ from
    \citep{varshney2011structural}. The full matrix is shown (top)
    along with a zoom-in to 14 neurons (center).  We wish to infer the
    corresponding weight matrix~$W$, an example of which is shown
    below.  \textbf{B} We also know the typial locations of the
    neurons \citep{white1986structure,wormatlas}. Given observed
    locations, we constrain possible assignments to neuron identities
    within~$\eta$ of the observed location.  \textbf{C} These
    constraints are represented as a matrix $\mathcal{C}^{(j)}$ for
    worm~$j$ which specifies possible assignments of observed neurons
    to known identities.  \textbf{D} To infer the weights, we must
    first infer the permutation $P^{(j)}$ that matches the observed
    neurons in worm~$j$ to the set of known identities.
    \textbf{E} The observed data is a matrix~$Y^{(j)}$ whose rows
    are ordered according to the order in which neurons were observed
    in that worm.  The permutation matrix maps this to the canonical
    ordering of the adjacency and weight matrices. Given~$\{Y^{(j)}\}_{j=1}^J$ and~$A$, we infer~$\{P^{(j)}\}_{j=1}^J$ and~$W$.}
\label{fig:1}
\end{figure*}

Ultimately, we are interested the dynamical system that governs how
neural activity evolves given its history and sensory inputs.
Bayesian methods are ideally suited to this goal, allowing us to
represent hierarchical probabilistic structures and integrate our
prior knowledge about the connectome, the locations of neurons, etc.
Bayesian learning and inference in dynamical systems with MCMC methods
is well-studied, even for complicated
models~\citep{Freitas2001,Paninski2010}. Furthermore, hierarchical
models to incorporate information from many worms are easily
constructed in a Bayesian framework~\citep{Gelman2014}.

However, our efforts to integrate information across worms are
complicated by a major hurdle: in practice, associating recorded
traces to neuron names is a painstaking, manual process.
Experimenters consider the location of the neuron along with its
pattern of activity to perform this matching, but the process is
laborious and the results are prone to error.  Without neuron names,
we cannot represent recordings canonically or learn about how one
neuron influences another. This technical problem prevents the
automatic use of hierarchical methods.

We present a method for overcoming this hurdle by incorporating
inference over permutations that match observed neurons
\textit{(neuron 1, neuron 2, ..., neuron~$N$)} to known names
\textit{(AVAL, AVAR, ..., SMDR)}.  Once the observed neurons have been
mapped to canonical names, we can learn about the shared dynamical
system. To start, we focus on a simple linear autoregressive
model for neural dynamics,
\begin{align}
  \widetilde{Y}_t^{(j)} &= (W \odot A) \widetilde{Y}_{t-1}^{(j)} + \epsilon_t^{(j)},
\end{align}
where~$W \in \reals^{N \times N}$ is the weight matrix we wish to infer;
$\odot$ denotes elementwise multiplication;
$A \in \{0,1\}^{N \times N}$ is the known adjacency matrix or connectome; 
and~$\widetilde{Y}_t^{(j)} \in \reals^N$ is the measured neural activity
at time~$t$ in worm~$j$.  The catch is that~$\widetilde{Y}_t^{(j)}$ is
assumed to be in canonical order; i.e. in the same order as the rows and
columns of~$W$ and~$A$. What we actually observe is,
\begin{align}
  Y_t^{(j)} &= P^{(j)} \widetilde{Y}_t^{(j)},
\end{align}
vectors that are permuted by matrix~$P^{(j)}$. In order to learn about~$W$,
we must also infer the permutation matrices. We assume~$\epsilon_t^{(j)} \sim \distNormal(0, \sigma^2 I)$ with known variance, and we place a Gaussian prior on~$W$.

The permutation matrices are constrained by side
information. Specifically, we use neural position along the worm's
body to constrain the possible neural identities for a given recorded
neuron. We only allow an observed neuron to be mapped to a known
identity if the observed location is within~$\eta$ of the expected
location.  This is illustrated in Fig.~\ref{fig:1}B. We represent
these constraints with the matrix~$\mathcal{C}^{(j)}$ so that
$\mathcal{C}^{(j)}_{mn}=1$ if and only if observed neuron $m$ is
within~$\eta$ of where canonical neuron $n$'s expected location.  An
example is shown in Figure \ref{fig:1}C. We let~$P^{(j)}$ have a
uniform prior over the set of matrices allowable under the given
constraints.

We need to perform joint posterior inference of $p(\{W,P^{(j)}\})$.
MCMC with simple Metropolis-Hastings proposals is straightforward, but
we found this mixed poorly in practice. Motivated by recent advances
in automatic variational inference \citep{Blei2017}, we considered
ways of extending this technique to permutation inference.  In section
\ref{sec:VI} we detail our VI formulation and summarize the methods we
developed. Then in section \ref{sec:results} we show that these
methods outperform alternatives.

 \section{New methods for variational inference of latent permutations}
  \label{sec:VI}
 Consider a latent variable model  determined by a prior over the latent $z\sim p(z)$ and a likelihood $p(y|z)$ for the observed data $y$. In the VI framework, instead of accessing the perhaps intractable posterior $p(z|y)$ one aims to find the distribution $q(z;\nu)$ among a certain variational family, parameterized by $\nu\in \mathcal{V}$, such that it minimizes its discrepancy with $p(z|y)$. Typically, one considers the KL divergence:
 \begin{equation} \nu^* = \argmin_{\nu\in\mathcal{V}} KL\left(p(z|y)\lVert q(z;\nu)\right).\end{equation}
In turn, one can show that the above problem is equivalent to the maximization of the \emph{evidence lower bound} (ELBO):
 \begin{equation}\label{eq:elbo} \nu^* = \argmax_{\nu\in\mathcal{V}} ELBO(q(z;\nu))\equiv  E_{q(z;\nu)}(\log p(y|z)) - KL(q(z;\nu)\lVert p(z)).\end{equation}
 To maximize equation ~\eqref{eq:elbo} one usually appeals to stochastic optimization methods \citep{Kushner1987}: specifically, all the expectations involved in ~\eqref{eq:elbo} are approximated by Monte Carlo samples, and gradient descent iterations are then performed to this approximation. One critical component is the choice of the Monte Carlo approximation. Perhaps the most common choice is through the so called \emph{score function estimator}, which bases upon the identity $h(\nu) \nabla_\nu \log h(\nu) =\nabla_\nu h(\nu)$. Unfortunately, this estimator, also referred to as REINFORCE \citep{Williams1992}, cannot be applied to permutations, since it involves the evaluation and differentiation of a likelihood which is intractable for any non-trivial distribution over permutations (computing the partition function involves a summation over N! terms).
 
An appealing alternative comes from the re-parameterization trick \cite{Kingma2013}, which leads to a new gradient estimator if one can re-parameterize $z$ as a differentiable function of a noise distribution and the parameters; i.e., if for certain $f$ and $\xi\sim p(\xi)$ one has $z=f(\xi,\nu)$. In the case of discrete random variables a re-parameterization always exists and it is given by the \emph{Gumbel trick} \citep{Papandreou2011,Balog2017}, which states that one can sample from any discrete distribution by perturbing each potential with Gumbel i.i.d noise, and then finding the configuration with the maximum value. Unfortunately, the underlying $f$ to this re-parameterization is the non-differentiable $\argmax$ operator, precluding the use of gradient descent methods.

Recent work by \citep{Jang2016,Maddison2016} proposed a solution to this problem, by replacing the $\argmax$ by a temperature ($\tau$) dependent $\mathrm{softmax}$ approximation, which in the limit  converges to the original $\argmax$. By combining the Gumbel trick with the softmax approximation, they conceived the \emph{Concrete} or \emph{Gumbel-Softmax} distribution, and obtain explicit distribution formulae. Then, they showed one can learn on a discrete latent variable model using the re-parameterization trick and gradient descent, by replacing the original ELBO with the surrogate arising by this continuous relaxation, as long as $\tau$ is chosen in a reasonable range: not too high as it would lead to a degenerate distribution in the simplex; but also not too low, to avoid too high variances of the gradients.

We developed three methods for extending the above to permutations. We name then \emph{stick-breaking}, \emph{rounding} and \emph{Gumbel-Sinkhorn} methods. We refer the reader to sections 3.1 and 3.2 of  \cite{Linderman2017} and section 4 of \cite{Anonymous2018learning} for details, respectively. Here we briefly summarize them: in all of them the primary geometric object is the Birkhoff polytope, the convex hull of permutation matrices, and analog to the probability simplex in this case. For the stick-breaking construction, we generalize to this polytope the one that exists in the simplex \citep{Linderman2015}, surmounting a new complication; of being able to consistently ``break the stick'' while satisfying both the row and column constrains that characterize a doubly stochastic matrix. For the rounding construction, we start by a noise distribution and force it to be close to permutation matrices by pulling them towards the extreme-points of the Birkhoff polytope. Finally, for the Gumbel-Sinkhorn method we notice that the so-called \emph{Sinkhorn operator}, or infinite and successive row and column normalization of a matrix, is a a natural extension of the softmax operator. With this, we are able to conceive the Gumbel-Sinkhorn distribution, which approximates the sampling of a relevant discrete distribution. Importantly, while stick-breaking and rounding yield explicit densities, Gumbel-Sinkhorn does not. However, there are ways to circumvent this difficulty, and overall we observe the latter performs the best.

\section{Results}
\label{sec:results}

We compared against three methods: (i) naive
variational inference, where we do not enforce the constraint that
$X^{(j)}$ be a permutation and instead treat each row of~$X^{(j)}$ as
a Dirichlet distributed vector; (ii) MCMC, where we alternate between
sampling from the conditionals of $W$ (Gaussian) and ${X^{(j)}}$, from
which one can sample by proposing local swaps, as described in
\cite{Diaconis2009}, and (iii) maximum a posteriori estimation (MAP).
Our MAP algorithm alternates between the optimizing estimate of $W$ given~$\{X^{(m)}, Y^{(m)}\}$ using linear regression and finding the optimal ${X^{(j)}}$. The second step requires solving a quadratic assignment
problem (QAP) in ${X^{(j)}}$; that is, it can be expressed as
$\mathrm{Tr}(AXBX^\trans)$ for matrices $A,B$. We used the QAP solver
proposed by~\citet{Vogelstein2015}.


We found that our method outperforms each
baseline, Specifically, we show that our method outperforms alternatives when there are many possible
candidates (Table 1) and when only a small proportion of neurons are known with
certitude (Table 2). 
Altogether, these results indicate our method enables a more efficient
use of information than its alternatives. This is consistent with
other results showing faster convergence of variational inference over
MCMC \citep{Blei2017}, especially with simple Metropolis-Hastings
proposals. We conjecture that MCMC could eventually obtain similar if
not better results, if current local proposals---swapping pairs of
labels--- were replaced by more involved ones.
\begin{table}[h!]
   \centering
  \begin{tabular}{lllllllllll}
    & \multicolumn{2}{c}{10} & \multicolumn{2}{c}{30} &   \multicolumn{2}{c}{45} & \multicolumn{2}{c}{60} \\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
& 1 worm & 4 worms & 1 Worm & 4 worms & 1 worm & 4 worms & 1 worms & 4 worms \\
    \midrule 
    NAIVE VI &.34 & .32 & .16 & .16 & .13 & .12 & .11 & .12 \\
   MAP   & .34 & .32  &.17 &.17& .14 & .13 & .13 & .12 \\
    MCMC   & .34 & .65  &.18 &.28& .14 & .17 & .13 & .15 \\
      %\makecell[l]{Gumbel-Sinkhorn \\ (no regularization)} 
     % &.77 & .92 & \textbf{.4} & .64 &  .25 & .44 & .21 & .39 \\
       %Rounding  (VI) &   .77  & .93 &  .33 &\textbf{.7} &  .18 & .48 & .17 & .37 \\
    VI   & \textbf{.79} & \textbf{.94} & \textbf{.4} & \textbf{.69} & \textbf{.25}&  \textbf{.51} & \textbf{.21} & \textbf{.44}\\
 
    \bottomrule
  \end{tabular}
   \caption{Accuracy in the C.elegans neural identification problem, for varying mean number of candidate neurons (10, 30, 45, 60) and number of worms.}
   \label{table:celeganssup}
\end{table}
\begin{table}[h]
   \centering
  \begin{tabular}{lllllllll}
    & \multicolumn{2}{c}{40.\%} & \multicolumn{2}{c}{30.\%} & \multicolumn{2}{c}{20.\%} & \multicolumn{2}{c}{10.\%}\\
    \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7} \cmidrule(lr){8-9}
     & $\eta=0.1$ & $\eta=0.2$ & $\eta=0.1$ & $\eta=0.2$  & $\eta=0.1$ & $\eta=0.2$  & $\eta=0.1$ & $\eta=0.2$ \\
    \midrule 
    Naive VI .43 & .41 & .33 & .31 & .23 & .22 & .12 & .1 \\
    MAP & .42 & .41  &.33 &.32& .23 & .22 & .12 & .11 \\
     MCMC   & .85 & .80  &.52 &.46& .3 & .26 & .15 & .12 \\
   %   \shortstack{Gumbel-Sinkhorn, no regularization (VI)} & .96 & .93 & .88 & .78 &  .69 & .52 & .39 & .21 \\
   %Rounding (VI)& \textbf{.97} & \textbf{.95} & \textbf{.90} & .84 & .75&  .\textbf{58} & \textbf{.37} & \textbf{.17} \\
    VI   & \textbf{.97} & \textbf{.96} & \textbf{.92} & \textbf{.84} & \textbf{.74} & \textbf{.58} & \textbf{.44} & \textbf{.23} \\
              \bottomrule
  \end{tabular}
   \caption{Accuracy in inferring true neural identity for different of proportion of known neurons, and two values of $\eta$. }%\todo[inline]{Gonzalo: Tell the reader what they should take away from these results.  It seems like this significantly outperforms Linderman et al. when there is significantly less data.  Also, with more data the differences don't neccesarily seem that significant.  Why is that the case?  What makes this method better in some circumstances than that one?}}
   \label{table:celegans}
\end{table}
\clearpage

\bibliography{refs}
\bibliographystyle{abbrvnat}

\end{document}
