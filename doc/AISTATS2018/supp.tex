\documentclass[twoside]{article}

\input{preamble.tex}
\input{preamble_math.tex}
\input{preamble_acronyms.tex}

\usepackage{blindtext}

\usepackage{aistats2018}

\DeclareRobustCommand{\parhead}[1]{\textbf{#1}~}

% If your paper is accepted, change the options for the package
% aistats2018 as follows:
%
%\usepackage[accepted]{aistats2018}
%
% This option will print headings for the title of your paper and
% headings for the authors names, plus a copyright note at the end of
% the first column of the first page.


\begin{document}

% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

\twocolumn[

\aistatstitle{Reparameterizing the Birkhoff Polytope for \\
  Variational Permutation Inference: Supplementary Material}

% \aistatsauthor{ Gonzalo E. Mena$^*$ \And Scott W. Linderman$^*$
%   \And  Hal Cooper \AND Liam Paninski \And John P. Cunningham }

% \aistatsaddress{ Columbia University}

\aistatsauthor{ Anonymous Authors }

\aistatsaddress{ Anonymous Institutions}
]
\appendix



\section{Alternative methods of discrete variational inference}

Recently there have been a number of proposals for extending the
reparameterization trick~\citep{rezende2014stochastic, Kingma2014} to
high dimensional discrete problems\footnote{Discrete inference is only
  problematic in the high dimensional case, since in low dimensional
  problems we can enumerate the possible values of~$x$ and compute the
  normalizing constant~$p(y) = \sum_x p(y, x)$.} by relaxing them to
analogous continuous problems \citep{maddison2016concrete,
  jang2016categorical, kusner2016gans}.  These approaches are based on
the following observation: if~$x \in \{0,1\}^N$ is a one-hot vector
drawn from a categorical distribution, then the support of~$p(x)$ is
the set of vertices of the~$N-1$ dimensional simplex.  We can
represent the distribution of~$x$ as an atomic density on the simplex.

\subsection{The Gumbel-softmax method}
Viewing~$x$ as a vertex of the simplex motivates a natural relaxation:
rather than restricting ourselves to atomic measures,
consider continuous densities on the simplex. To be concrete, suppose
the density of~$x$ is defined by the transformation,
\begin{align*}
  \xi_n &\iid{\sim} \mathrm{Gumbel}(0, 1) \\
  \psi_n & = \log \theta_n + \xi_n  \\
  x &=  \mathsf{softmax}(\psi / \tau) \\
        &=\left(\frac{e^{\psi_1 / \tau}}{\sum_{n=1}^N e^{\psi_n / \tau}},
      \,\ldots,\,
      \frac{e^{\psi_N / \tau}}{\sum_{n=1}^N e^{\psi_n / \tau}} \right).
\end{align*}
The output~$x$ is now a point on the simplex, and the
parameter~${\theta = (\theta_1, \ldots, \theta_N)}$ can be optimized
via stochastic gradient ascent with the reparameterization trick.

The Gumbel distribution leads to a nicely interpretable model:
when~$\theta$ is a probability mass function, adding Gumbel noise and
taking the argmax yields an exact sample from~$\theta$; the softmax is
a natural relaxation. As the temperature~$\tau$ goes to zero, the
softmax converges to the argmax function. Ultimately, however, this is
just a continuous relaxation of an atomic density to a continuous
density.

Stick-breaking and rounding offer two alternative ways of conceiving a
relaxed version of a discrete random variable, and both are amenable
to reparameterization. However, unlike the Gumbel-Softmax, these
relaxations enable extensions to more complex combinatorial objects,
notably, permutations.

\subsection{Stick-breaking}

The stick-breaking transformation to the Birkhoff polytope presented
in the main text contains a recipe for stick-breaking on the simplex.
In particular, as we filled in the first row of the doubly-stochastic
matrix, we were transforming a real-valued vector~$\psi \in \reals^{N-1}$
to a point in the simplex.  We present this procedure for
discrete variational inference again here in simplified form.
Start with a reparameterization of a Gaussian vector,
\begin{align*}
  \xi_n &\iid{\sim} \distNormal(0, 1), \\
  \psi_n & = \mu_n + \eta_n \xi_n, \qquad 1 \leq n \leq N-1,
\end{align*}
parameterized by~${\theta = (\mu_n, \eta_n)_{n=1}^{N-1}}$. 
Then map this to a point in the simplex:
\begin{align*}
  x_1 &= \sigma(\psi_1 / \tau), \\
  x_n &= \sigma(\psi_n / \tau) \left(1- \sum_{m=1}^{n-1} x_m\right), \qquad 2 \leq n \leq N-1,  \\
  x_N &= 1- \sum_{m=1}^{N-1} x_m,
\end{align*}
where~${\sigma(u) = (1+e^{-u})^{-1}}$ is the logistic
function. Here,~$\sigma(\psi_n/\tau)$ is the fraction of the remaining
``stick'' of probability mass assigned to~$x_n$.  This procedure is
invertible, the Jacobian~$\frac{\mathrm{d}x}{\mathrm{d}\psi}$ is
lower-triangular, and the determinant of the Jacobian is easy to
compute.  \citet{linderman2015dependent} compute the density of~$x$
implied by a Gaussian density on~$\psi$.

The temperature~$\tau$ controls how concentrated $p(\pi)$ is at the
vertices of the simplex, and with appropriate choices of parameters,
in the limit ~$\tau \to 0$ we can recover any categorial
distribution. In the other limit, as ~$\tau \to \infty$, the density
concentrates on a point in the interior of the simplex determined by
the parameters, and for intermediate values, the density is continuous
on the simplex.

Finally, note that the logistic-normal construction only one possible
choice.  We could instead
let~${\psi_n \sim \mathrm{Beta}(\tfrac{a_n}{\tau}, \tfrac{b_n}{\tau})}$
and~${x_n = \psi_n}$. This would lead to the Dirichlet distribution on
the simplex.  The beta distribution is slightly harder to
reparameterize since it is typically simulated with a rejection
sampling procedure, but~\citet{naesseth2017reparameterization} have
shown how this can be handled with a mix of reparameterization and
score-function gradients.  Alternatively, the beta distribution could
be replaced with the Kumaraswamy distribution, which is quite similar
to the beta distribution but is easily reparameterizable.

\subsection{Rounding}
Rounding transformations also have a natural analog for discrete
variational inference.  Define the rounding operator,
\begin{align*}
  \mathsf{round}(\psi)
  &= \argmin_{e_n} \| e_n - \psi \|^2,
\end{align*}
which maps~${\psi \in \reals^N}$ to the one-hot vectors~${e_n}$;
i.e. the vectors in~${\{0,1\}^N}$ with~$n$-th entry equal to one and
all other entries equal zero.  This is equivalent to
defining~${\mathsf{round}(\psi) = e_{n^*}}$ where
\begin{align*}
  n^* &= \argmin_{n} \|e_n - \psi \|^2 \\
  &= \argmin_{n} \sum_{m \neq n} \psi_m^2 + (1 - \psi_n)^2  \\
  &= \argmin_{n} \sum_{m \neq n} \psi_m^2 + \psi_n^2 - 2\psi_n + 1 \\
  &= \argmin_{n} \|\psi\|^2 - 2\psi_n + 1 \\
  &= \argmax_n \psi_n.
\end{align*}
In the case of a tie, let~$n^*$ be the smallest index~$n$ such
that~$\psi_n > \psi_m$ for all~$m < n$. Rounding effectively
partitions the space into~$N$ disjoint ``Voronoi'' cells,
\begin{align*}
  V_n &= \Big \{ \psi \in \reals^N : \,
        \psi_n \geq \psi_m \, \forall m \; \wedge \;
        \psi_n > \psi_m \, \forall m < n
        \Big \}.
\end{align*}
By definition,~${\mathsf{round}(\psi) = e_{n^*}}$ for
all~${\psi \in V_{n^*}}$


We define a map that pulls points toward their rounded values,
\begin{align}
  \label{eq:round}
  x &=  \tau \psi + (1-\tau) \mathsf{round}(\psi).
\end{align}

\begin{proposition}
  \label{prop:round}
  For~${\tau \in [0,1]}$, the map defined by~\eqref{eq:round} moves
  points strictly closer to their rounded values so
  that~$\mathsf{round}(\psi) = \mathsf{round}(x)$.
\end{proposition}

\begin{proof}
  Note that the Voronoi cells are defined by linear inequalities,
  making them convex sets.  Since~$x$ is a convex combination
  of~$\psi$ and~$e_{n^*}$, both of which belong to the convex
  set~$V_{n^*}$, $x$~must belong to~$V_{n^*}$ as well.
\end{proof}

Similarly,~$x$ will be a point on the simplex if an only if~$\psi$ is
on the simplex as well.  By analogy to the rounding transformations
for permutation inference, in categorical inference we use a Gaussian
distribution~${\psi \sim \distNormal(\mathsf{proj}(m), H)}$,
where~$\mathsf{proj}(m)$ is the projection of~$m \in \reals_+^N$ onto
the simplex.  Still, the simplex has zero measure under the Gaussian
distribution.  It follows that the rounded points~$x$ will almost
surely not be on the simplex either.  The supposition of this approach
is that this is not a problem: relaxing to the simplex is nice but not
required.

In the zero-temperature limit we obtain a discrete distribution on the
vertices of the simplex.  For~${\tau \in (0,1]}$ we have a
distribution on~${\mcX_\tau \subseteq \reals^N}$, the subset of the
reals to which the rounding operation maps. (For~${0 \leq \tau < 1}$
this is a strict subset of~$\reals^N$.) To derive the
density~$q(x)$, we need the inverse transformation and the
determinant of its Jacobian.  From Proposition~\ref{prop:round}, it
follows that the inverse transformation is given by,
\begin{align*}
  \psi &= \frac{1}{\tau} x - \frac{1 - \tau}{\tau} \mathsf{round}(x).
\end{align*}
As long as~$\psi$ is in the interior of its Voronoi cell,
the~$\mathsf{round}$ function is piecewise constant and the
Jacobian is~${\tfrac{\mathrm{d}\psi}{\mathrm{d}x} = \tfrac{1}{\tau} I}$,
and its determinant is~$\tau^{-N}$. Taken together, we have,
\begin{multline*}
  q(x; m, H) =  \\
  \tau^{-N} \distNormal \left(\frac{1}{\tau}x - \frac{1-\tau}{\tau} \mathsf{round}(x); \, \mathsf{proj}(m), H \right) \\
  \times \bbI[x \in \mcX_\tau].
\end{multline*}
Compare this to the density of the rounded random variables for
permutation inference. 

\section{Limit analysis for stick-breaking}

Here we state and prove that for  the stick-breaking we consider here, we can arrive to either arbitrary points in the i) simplex or ii) to any categorical distribution as limiting cases (in the temperature).  First, we need some lemmas.


\textbf{Lemma 1.}  The following statements are true:
\begin{enumerate} \item the degenerate case where  $z_k$ is deterministic leads to $\pi\sim \delta(\tilde{\pi})$  (i.e, single atom in the point $\tilde{\pi}$). Also, if $z_k$ can be any in $(0,1)$ then any deterministic $\pi$ in the interior of the simplex can be realized.
\item the degenerate case where  $z_k$ are Bernoulli with parameter $p_k(\theta) \in (0,1)$ leads to $\pi$ having an atomic distribution with atoms in the vertices of $\Delta^{k-1}$; i.e, $\pi$ is categorical. We have the following expression for the probabilities of the atoms $\pi_k=1$ (one hot vectors):
\begin{align}
\label{eq:onehotprob}
\nonumber P(\pi_k =1)&= \prod_{i=1}^{k-1} (1-p_i(\theta)) p_k(\theta)  \;\; \ k=2, \ldots, K-1,\\  \nonumber & P(\pi_K =1) = \prod_{i=1}^{K-1} (1-p_i(\theta)).
\end{align}

Moreover, if for each index $k$ any parameter of the Bernoulli variable $z_k$ can be realized through appropriate choice of $\theta$, then any categorical distribution can be realized.

\end{enumerate}
\textit{Proof}: (a) both claims are obvious and come from the invertibility of the function $\mathcal{SB} \circ h (\cdot)$. (b) the formulae for $P(\pi_k =1)$ comes from expressing the event $\pi_k=1$ equivalently as $\pi_k=1,\pi_i=0, i<k$ and then, conditioning backwards successively. The second statement comes from the following expression, which easily follows  from \eqref{eq:onehotprob}:
$$ p_k(\theta)=\frac{P(\pi_k =1)}{P(\pi_{k-1} =1)}\frac{p_{k-1}(\theta)}{1-p_{k-1}(\theta)},\quad k =1,\ldots, K-1.$$
The recursive nature of the above equation gives a recipe to iteratively determine the required $p_k(\theta)$, given  $P(\pi_k =1), P(\pi_{k-1} =1)$ and the already computed $p_{k-1}(\theta)$.


Now we can state our results:

\textbf{Lemma 2.} If $z=\sigma(\psi),\psi\sim\mathcal{N}(\mu,\eta^2)$, then
\begin{enumerate} \item the  limit $\eta\rightarrow 0$ and $\mu$ fixed leads to the deterministic $z=\sigma(\mu)$. 
\item the limit $\mu\rightarrow \infty, \eta^2=\mu/K$ with K constant leads to $z\sim \text{Bernoulli}(\Phi(K))$, with $\Phi(\cdot)$ denoting the standard normal cdf.
\end{enumerate} In both cases the convergence is in distribution 

\textit{Proof}. The first convergence is obvious. To see the second, let's index $\mu_n$ and  study the cdf $F$ of $z_n$ on the interval (0,1) (it evaluates zero below zero and one above one).
\begin{align}F_{z_n}(x)&= P(\sigma(\psi_n)<x) \\
&=P(\psi_n< \sigma^{-1}(x))\\
&=P(\mu_n +\mu_n/K\xi <\sigma^{-1}(x)),\\\
&= P( \xi <\sigma^{-1}(x)K/\mu_n - K)\\
&= \Phi( \sigma^{-1}(x)K/\mu_n - K) 
\end{align}

Therefore, by continuity of $\Phi$ we obtain $F_{\Psi_n}(x)\rightarrow \Phi(-K)$ for all points $x\in(0,1)$. On the other hand, the cdf of a Bernoulli random $F$ variable is given by  a step function that abruptly changes at zero, from zero to $1-p$, and at one, from $1-p$ to 1. As convergence occurs at all continuity points (the interval $(0,1)$), we conclude (recall, $1-p= \Phi(-K)\rightarrow \Phi(K)=p$). Notice that the above representation only allows  to converge to $p>0.5$, as $K$ has to be positive. This can be fixed by choosing sequence with negative $\mu$ instead.

\textbf{Proposition.} For the stick-breaking construction, any arbitrary distribution can be realized in the low-temperature limit. Also, in the high-temperature limit convergence is to certain point(s) in the interior of the simplex.

\textit{Proof}: Consider each distribution separately

We have $z_k = \sigma\left( \frac{\mu_k+\eta_k\xi}{\tau}\right)$, in the low temperature case use Lemma 2 (b) by the always available representation  $K= \frac{\mu}{\eta^2}$and conclude by Lemma 1(b). In the high temperature case convergence is to the point $\pi = \mathcal{SB}(0.5,0.5,\ldots, 0.5)$.


%\subsection{Rounding}
%Here we have two extremes: at $\tau =1$ we obtain a continuous distribution in the space (here, Gaussian). If $\tau=0$ the resulting distribution has only atoms in the one-hot vectors $p_n$, in this proof assumed to be the one-hot vectors. We show that in this case it is possible to represent any arbitrary categorical distribution through a judicious choice of the parameters.


%\textbf{Proposition:} In the zero temperature case, i.e., $\pi  = R^\mathcal{P}(\psi)$ it is possible to represent any arbitrary distribution i.e, for any $\alpha$ in the $N-1$ simplex there exists Gaussian parameters $(\mu, \eta)$ so that   $P(\pi = p_n)  = \alpha_n$. Points inside the simplex are realized directly, while distributions with some $\alpha_k=0$ are realized through a limiting process in the parameters.

%\textit{Proof:} First set $\eta_n=1$. By representing $\psi = \mu + \xi$ where $\xi\sim\distNormal(0, I)$ we see that
%$$\alpha_n = P(R^\mathcal{P}(\mu + \xi) = p_n ) = P(\mu + \xi \in V^\mathcal{P}_{n}) = P(\xi \in V^\mathcal{P}_{n} - \mu) =  \int_{V^\mathcal{P}_{n} - \mu} \frac{1}{(2\pi)^\frac{N}{2}}e^{-\frac{||x||^2}{2}} dx.$$
%Three conclusions are drawn from the above: first, we see that probabilities are ultimately Gaussian integrals over a new partition, a translation of the Voronoi regions $V^\mathcal{P}_{n}$. Second,
%the map $(\mu_1,\ldots,  \mu_n)\xrightarrow{m} (\alpha_1,\ldots, \alpha_n)$ is continuous by virtue of the dominated convergence theorem \cite{browder2012mathematical}: indeed, if $\mu_i\rightarrow \mu$ then $(\alpha^i_1,\ldots, \alpha^i_n) \rightarrow (\alpha_1,\ldots, \alpha_n)$, as the $\alpha^i_n$ are integrals that can be expressed using indicator functions in the integrands (which are all bounded by the integrable Gaussian density), and as pointwise convergence of the indicators holds because of the continuity of the translation operator $f_\mu(\cdot) = \cdot + \mu$. Third, for each $q\in(0,1)$ it is possible to choose $\mu$ so that $\alpha_n =q$ and $\alpha_m = (1-q)/(n-1)$. Indeed, by moving $\mu_n$ between $-\infty$ and $\infty$ while keeping the other $\mu_k$ fixed then $V^\mathcal{P}_{n} - \mu$ fluctuates between the empty set ($\alpha_n =0$) and the entire space ($\alpha_n=1$). Therefore, by the continuity of $m$ and the intermediate value theorem, every value in $\alpha_n\in (0,1)$ is realized, and by symmetry the other $\alpha_k$ occupy the remaining mass uniformly. 

%To conclude, we use the fact that the image of a convex set through a continuous function is also convex \cite{Rockafellar70}. We also showed that for each tolerance $\epsilon$ the image of $m$ contains these $\epsilon$-one-hot vectors: that is, the points with $(1-\epsilon)$ in one coordinate and $\epsilon/ (n-1)$ in the rest. Then, given a point in the interior of the simplex choose $\epsilon$ small enough so it is in the convex hull of the $\epsilon$-one vectors. Then, by the above theorem there must be a pre-image $\mu$ that realizes this point.

%For $\alpha$ with zero entries the above arguments has to be extended to permit a limit process where $\mu$ goes to either infinity or infinity. It is easy to see that by that extension it is possible to represent any $\alpha$ in the border of the simplex.


\section{Variational Autoencoders (VAE) with categorical latent variables}



We considered the density estimation task on MNIST digits, as in
\cite{maddison2016concrete, jang2016categorical}, where observed
digits are reconstructed from a latent discrete code. We used the
continuous ELBO for training, and evaluated performance based on the
marginal likelihood, estimated through the multi-sample variational
objective of the discretized model. We compared against the methods of
\cite{jang2016categorical, maddison2016concrete}, finding similar
(although slightly worse) results (Table~\ref{tab:vae}). This difference may be
interpreted as the price to be paid in order to enable an extension of
a relaxed distribution over categories, to permutations.

\begin{table}[h]
  \caption{Summary of results in VAE}
  \label{tab:vae}
  \centering
  \begin{tabular}{ll}
    \textbf{Method} & $- \log p(x)$ \\
    \hline
    Gumbel-Softmax    & 106.7 \\
    Concrete  &  111.5\\
    Rounding &  121.1 \\
    Stick-breaking & 119. 8\\
    \bottomrule
  \end{tabular}
\end{table}


% \begin{table*}[t]
%   \caption{Battacharya distances in the synthetic matching experiment}
%   \label{sample-table}
%   \centering
%   \begin{tabular}{llllllll}
%    & \multicolumn{1}{c}{Rounding} & \multicolumn{1}{c}{Stick-breaking} & \multicolumn{5}{c}{Mallows}\\
%     \cmidrule(lr){2-2} \cmidrule(lr){3-3} \cmidrule(lr){4-8}
%     &    & &   $\theta=0.1$ &  $\theta=1$ & $\theta=2$ & $\theta=5$ & $\theta=10$ \\
%     \midrule
%     $\sigma=0.1$     & .06 & .09  &.93 &.51& .23  & .08 &.08\\
%     $\sigma=0.25$     & .21 & .23 & .92 &.53 & .33&  .27 &.27\\
%      $\sigma=0.5$     & .32 & .41 & .89 &.61 & .53&  .54& .54\\
%      $\sigma=0.75$     & .38   & .55 & .85 &.71 & .69&  .72 &.72\\
   
%     \bottomrule
%   \end{tabular}
% \end{table*}

\subsection*{MNIST reconstructions}
In figure \ref{fig:VAE} we show some MNIST  reconstructions using Gumbel-Softmax, stick-breaking and rounding reparameterizations. In all the three cases reconstructions are reasonably accurate, and there is diversity in reconstructions.
\begin{figure*}[t]
  \centering
  \includegraphics[width=5.in]{../figures/figure4.pdf} 
  \caption{Examples of true and reconstructed digits from their corresponding random codes using with $N=20$ categorical variables with $K=10$ possible values.
  }
\label{fig:VAE}
\end{figure*}


\section{Variational inference for Permutation details}

\subsubsection*{Continuous prior distributions.} 
Continuous relaxations requires
re-thinking of the objective. As in \cite{maddison2016concrete}, we
maximize a relaxed ELBO, for which we need to specify a new continuous
prior $p(x)$ over the latent variables. Moreover, it is critical to conceive sensible priors for permutations, that could serve in a variational inference routine to penalize configurations that are away from permutation matrices (i.e. close to the barycenter of the Birkhoff polytope).

For our categorical experiment on MNIST we use a mixture of Gaussians around
each vertex,~${p(x) = \tfrac{1}{N} \sum_{n=1}^N \mathcal{N}(x \given e_k, \eta^2)}$. 
This can be extended to permutations, where use a mixture of Gaussians for each
dimension,
\begin{align}
\label{eq:permprior}
  \nonumber p(X) &= \prod_{m=1}^N \prod_{n=1}^N
  \frac{1}{2} \left(\mathcal{N}(x_{mn} \given 0, \eta^2) + \distNormal(x_{mn} \given 1, \eta^2 \right).
  \end{align}
Although this prior puts significant mass invalid points
(e.g.~$\bone$), it penalizes $X$ that far from~$\mcB_N$.

\subsubsection*{Deriving an expression for the ELBO}
Here we show that if $X=G(\Xi;\theta)$ with $G$ differentiable one can evaluate the second term in equation \eqref{eq:elbo} \footnote{Notice that we uppercase the variables in \eqref{eq:elbo} this is in consistency to our notation in section \ref{sec:permutation}}. Moreover, here, to exploit the similarities between both methods (stick-breaking and rounding), we further factor $G$ into two functions: $G=H \circ F$, $X = H(\Psi)$ and $\Psi = F(\Xi; \theta)$ (both $H,F$ invertible). This means all dependency of $X$ in the parameters is through $\Psi$. Under this assumption, and denoting $\Psi\sim p(\Psi,\theta)$, the second term in equation \eqref{eq:elbo} (without gradient) can be computed as:
 then \begin{align} \nonumber
\E_{r(\Xi)} \left[- \log q(G(\Xi;\theta)); \theta) \right] = & \bbH(\Psi; \theta) +\\ \nonumber & E_{r(\Xi)}\left[\log | DH(F(\Xi,\theta))|\right].\end{align}
\textbf{Proof}:
Indeed, first, it is obvious that $$\E_{r(\Xi)} \left[- \log q(G(\Xi;\theta)); \theta) \right] = \E_{r(\Xi)} \left[- \log q(H(F(\Xi,\theta)); \theta) \right] $$
Then, by the `Law of the Unconscious Statistician' we have:
 $$\E_{r(\Xi)} \left[- \log q(H(F(\Xi,\theta)); \theta) \right] = \E_{p(\Psi;\theta)} \left[- \log q(H(\psi); \theta) \right]. $$
Now, by the change of variable theorem and derivative and determinant inversion rules, we obtain ($D$ means the Jacobian, the matrix of derivatives) :\begin{align}
\nonumber q(H(\Psi); \theta) & = p(H^{-1}(X) ;\theta)  |DH ^{-1}(X) | \\ \nonumber
 & = p(\Psi; \theta) | DH (\Psi) | ^{-1}.
 \end{align}
 To conclude we use once more the Law of the Unconscious Statistician:
 \begin{eqnarray}
\nonumber \E_{r(\Xi)} \left[- \log q(G(\Xi;\theta)); \theta) \right]  &= \E_{p(\Psi;\theta)}\left[ - \log p(\Psi;\theta)\right] +   \\ & \nonumber \E_{p(\psi;\theta)}\left[\log | DH(\psi)|\right] \\ \label{eq:elbo2} 
 &= \bbH(\Psi; \theta)+ \\ \nonumber & E_{r(\xi)}\left[\log | DH(F(\Xi;\theta))|\right].\end{eqnarray}
 

\subsubsection*{Estimating the ELBO} 
Here we describe how to compute each of terms of equation \eqref{eq:elbo2}, needed for ELBO computations. First, as $\Psi$ is Gaussian for both rounding and stick-breaking, the entropy term is straightforward and equal to $N\log(\eta^2 2\pi e )/2$ ($\eta$ may depend on the temperature and depends on the method). 

Notice that to state $\Psi$ is Gaussian in the stick-breaking case we slightly deviate from \ref{sec:permutation}. Specifically, here we call $\Psi=\frac{\mu_{mn} + \eta_{mn} \Xi_{mn}}{\tau} $ and define $\Psi' = \sigma(\Psi)$.

The second term of equation \eqref{eq:elbo2} is estimated using Monte-Carlo samples, and its derivation depends on the method. 

\textbf{Rounding}

Here $H$ is piecewise linear: the set of discontinuities (border of the 'Voronoi cells' associated to each permutation) has Lebesgue measure zero. So we can still apply the change of variables theorem. Therefore, 
$\log |DH(F(\Xi;\theta)) |= N\log
\tau$. This means we don't even need to take samples to compute this term.


\textbf{Stick-breaking}

It is important to note that the transformation $H$ that maps $\Psi'\rightarrow X$ is only piecewise
continuous: the function is not differentiable at the points where
the bounds change; for example, when changing~$\Psi'$ causes the
active upper bound to switch from the row to the column constraint
or vice versa.  

Notice that these bounds only depend on values of~$X$ that
have already been computed; i.e., those that are above or to the left of
the~$(i,j)$-th entry. Thus, the transformation from ~$\Psi'$ to~$X$
is feed-forward according to this ordering.  Consequently, the
Jacobian of the inverse transformation $H^{-1}$ ,~$\mathrm{d}\Psi' / \mathrm{d} X$,
is lower triangular, and its determinant is the product of its diagonal,
\begin{align}
\nonumber \left| \frac{\mathrm{d} \Psi'} {\mathrm{d} X} \right|
&= \prod_{i=1}^{n-1} \prod_{j=1}^{n-1} \frac{\partial \psi_{ij} }{\partial {\pi}_{ij}} \\
\nonumber &= \prod_{i=1}^{n-1} \prod_{j=1}^{n-1} \frac{\partial}{\partial {\pi}_{ij}}
\sigma^{-1} \left( \frac{{\pi}_{ij} - \ell_{ij}}{u_{ij} - \ell_{ij}} \right ) \\
\nonumber &= \prod_{i=1}^{n-1} \prod_{j=1}^{n-1}
\left( \frac{1}{u_{ij} - \ell_{ij}} \right )
\left( \frac{u_{ij} - \ell_{ij}}{{\pi}_{ij} - \ell_{ij}} \right )
\left( \frac{u_{ij} - \ell_{ij}}{u_{ij} - {\pi}_{ij}} \right ) \\
\nonumber &= \prod_{i=1}^{n-1} \prod_{j=1}^{n-1}
\frac{u_{ij} - \ell_{ij}}{({\pi}_{ij} - \ell_{ij}) (u_{ij} - {\pi}_{ij})}
\end{align}

To compute the gradient of the forward transformation $H$ one simply needs to invert the above (or put a negative sign, in the logarithm scale). Finally,  to incorporate the effect of $\sigma$ ($\Psi'=\sigma(\Psi)$, by the chain rule,  one only needs to add a term corresponding to this derivative, $d\sigma(x)/dx=\sigma(x)\sigma(-x)$. 
\subsection*{Experiment details}

Experiments were run on a High Performance Computing (HPC) cluster, allowing the execution of hundreds of processes in parallel to efficiently determine best hyperparameter configurations.

For experiments with Variational Auto-encoder we used Tensorflow \citep{Abadi2016}, slightly changing the code made available in conjunction with \cite{Jang2016}. For experiments on synthetic matching and the C. elegans example we used Autograd  \citep{maclaurin2015autograd}, explicitly avoiding propagating gradients through the non-differentiable operation of solving a matching problem (the $\mathsf{round}$ in \ref{sub:rounding}).

In all experiment we used the ADAM as optimizer, with learning rate 0.1. For rounding, the parameter vector $H$ defined in \ref{sub:rounding}(iii) was constrained to lie in the interval $[0.1, 0.5]$. Also, for rounding, we used ten iterations of the Sinkhorn-Knopp algorithm, to obtain points in the Birkhoff polytope. For stick-breaking the variances $\nu$ defined in \ref{sub:stickbreaking} was constrained between $1e-8$ and 1.0. In either case, the temperature parameter was calibrated using a grid search.
 
In the C. elegans example we considered the symmetrized version of the adjacency matrix described in \citep{varshney2011structural} (i.e. we used $A'=(A+A^\top)/2$, and the matrix $W$ was chosen antisymmetric, with entries sampled randomly with the sparsity pattern dictated by $A'$. To avoid divergence, the matrix $W$ was then re-scaled by 1.1 times its spectral radius. This choice, although not essential, induced a reasonably well behaved linear dynamical system, rich in non-damped oscillations. We used a time window of $T=1000$ time samples, and added spherical standard noise at each time 


\bibliography{refs}
\bibliographystyle{abbrvnat}


\end{document}
